{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the array ephys workflow, we need to properly set up the DataJoint configuration. The configuration will be saved in a file called `dj_local_conf.json` on each machine and this notebook walks you through the process.\n",
    "\n",
    "\n",
    "**The configuration only needs to be set up once**, if you have gone through the configuration before, directly go to [02-workflow-structure](02-workflow-structure-optional.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up configuration in root directory of this package\n",
    "\n",
    "As a convention, we set the configuration up in the root directory of the workflow package and always starts importing datajoint and pipeline modules from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure database host address and credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up the host, user and password in the `dj.config` global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "dj.config['database.host'] = '{YOUR_HOST}'\n",
    "dj.config['database.user'] = '{YOUR_USERNAME}'\n",
    "dj.config['database.password'] = getpass.getpass() # enter the password securily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to connect to the database at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the `custom` field in `dj.config` for the element-array-ephys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major component of the current workflow is the [DataJoint Array Ephys Element](https://github.com/datajoint/element-ephys). Array Ephys Element requires configurations in the field `custom` in `dj.config`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database prefix\n",
    "\n",
    "Giving a prefix to schema could help on the configuration of privilege settings. For example, if we set prefix `neuro_`, every schema created with the current workflow will start with `neuro_`, e.g. `neuro_lab`, `neuro_subject`, `neuro_ephys` etc.\n",
    "\n",
    "The prefix could be configurated as follows in `dj.config`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config['custom'] = {'database.prefix': 'neuro_'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root directories for raw ephys data and kilosort 2 processed results\n",
    "\n",
    "+ `ephys_root_data_dir` field indicates the root directory for the **ephys raw data** from SpikeGLX or OpenEphys (e.g. `*imec0.ap.bin`, `*imec0.ap.meta`, `*imec0.lf.bin`, `imec0.lf.meta`) or the **clustering results** from kilosort2 (e.g. `spike_times.npy`, `spike_clusters.npy`). The root path typically **do not** contain information of subjects or sessions, all data from subjects/sessions should be subdirectories in the root path.\n",
    "\n",
    "In the example dataset downloaded with [this instruction](00-data-download-optional.ipynb), `/tmp/test_data` will be the root\n",
    "\n",
    "```\n",
    "/tmp/test_data/\n",
    "- subject6\n",
    "    - session1\n",
    "        - towersTask_g0_imec0\n",
    "        - towersTask_g0_t0_nidq.meta\n",
    "        - towersTask_g0_t0.nidq.bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is only one root path.\n",
    "dj.config['custom']['ephys_root_data_dir'] = '/tmp/test_data'\n",
    "# If there are multiple possible root paths:\n",
    "dj.config['custom']['ephys_root_data_dir'] = ['/tmp/test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In the database, every path for the ephys raw data is **relative to this root path**. The benefit is that the absolute path could be configured for each machine, and when data transfer happens, we just need to change the root directory in the config file.\n",
    "+ The workflow supports **multiple root directories**. If there are multiple possible root directories, specify the `ephys_root_data_dir` as a list.\n",
    "+ The root path(s) are **specific to each machine**, as the name of drive mount could be different for different operating systems or machines.\n",
    "+ In the context of the workflow, all the paths saved into the database or saved in the config file need to be in the **POSIX standards** (Unix/Linux), with `/`. The path conversion for machines of any operating system is taken care of inside the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the configuration as a json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the proper configurations, we could save this as a file, either as a local json file, or a global file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config.save_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local configuration file is saved as `dj_local_conf.json` in the root directory of this package `bl_pipeline_python`. Next time if you change your directory to `bl_pipeline_python` before importing datajoint and the pipeline packages, the configurations will get properly loaded.\n",
    "\n",
    "If saved globally, there will be a hidden configuration file saved in your root directory. The configuration will be loaded no matter where the directory is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dj.config.save_global()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the configuration, we will be able to review the workflow structure with [02-workflow-structure-optional](02-workflow-structure-optional.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bl_dev",
   "language": "python",
   "name": "bl_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
